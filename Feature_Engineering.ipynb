{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOhteljTllenOIpxG+l4YM5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jaidatta71/Chatbot/blob/main/Feature_Engineering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This assignment focuses on solving a specific regression problem using basic cross-validation with a train/test/validation split. In addition to using the methods explored, this assignment also aims to familiarize you with further utilities for data transformation including, the OneHotEncoder and OrdinalEncoder along with their use in a make_column_transformer.\n",
        "\n",
        "The operations of encoding categorical features will be introduced using sklearn. This will allow you to streamline your model-building pipelines. Depending on whether a string type feature is ordinal or categorical we want to encode differently. The OrdinalEncoder will be used to encode features that do not need to be binarized due to an underlying order, and OneHotEncoder for categorical features (as a similar approach to that of the .get_dummies() method in pandas). By the end of the assignment, you will see how to chain multiple feature encoding methods together, including the earlier PolynomialFeatures for numeric features."
      ],
      "metadata": {
        "id": "ExmQcJHveKn5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GUOCIDLsZjk-"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import PolynomialFeatures, OneHotEncoder, OrdinalEncoder\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.compose import make_column_transformer, make_column_selector\n",
        "\n",
        "from sklearn import set_config\n",
        "\n",
        "set_config(display=\"diagram\") #setting this will display your pipelines as seen above"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **The Data:** Ames Housing\n",
        "This dataset is a popular beginning dataset used in teaching regression. The task is to use specific features of houses to predict the price of the house. In addition to this, as discussed in video 8.10 -- this dataset is available for use in an ongoing competition where you can use the test.csv to submit your model's predictions. Accordingly, the two data files are identical with the exception of the test.csv file not containing the target feature.\n",
        "\n",
        "The data contains 81 columns of different information on the individual houses and their sale price. A full description of the data is attached here. In this assignment, you will use a small subset of the features to begin modeling with that includes ordinal, categorical, and numeric features. As an optional exercise, you are encouraged to continue engineering additional features and attempt to improve the performance of your model including submitting the predictions on Kaggle."
      ],
      "metadata": {
        "id": "SMQ8G7BCeAwA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_csv('data/train.csv')\n",
        "test = pd.read_csv('data/test.csv')\n",
        "train.info()"
      ],
      "metadata": {
        "id": "Xf8Yz-fhZrGU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#note the difference in one column from train to test\n",
        "[i for i in train.columns if i not in test.columns]"
      ],
      "metadata": {
        "id": "7nICLRcwZtE_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = train.drop('SalePrice', axis = 1)\n",
        "y = train['SalePrice']\n",
        "print(type(X))\n",
        "print(type(y))"
      ],
      "metadata": {
        "id": "d36mC9PUZyd1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem 1\n",
        "# Train/Test split\n",
        "\n",
        "Despite having a test dataset, you want to create a holdout set to assess your model's performance. To do so, use sklearn's train_test_split to split X and y with arguments:\n",
        "\n",
        "*   test_size = 0.3\n",
        "*   random_state = 22\n",
        "\n",
        "Assign your results to X_train, X_test, y_train, y_test."
      ],
      "metadata": {
        "id": "RcwB_RSBhAdP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### GRADED\n",
        "\n",
        "X_train, X_test, y_train, y_test =  train_test_split(X, y, test_size=0.3, random_state=22)\n",
        "\n",
        "# YOUR CODE HERE\n",
        "#raise NotImplementedError()\n",
        "\n",
        "# Answer check\n",
        "print(X_train.shape)\n",
        "print(X_test.shape)\n",
        "print(type(X_train), type(y_train))#should be DataFrame and Series"
      ],
      "metadata": {
        "id": "KAYgrvrvZ12x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem 2\n",
        "# Baseline Predictions\n",
        "\n",
        "\n",
        "Before building a regression model, you should set a baseline to compare your later models to. One way to do this is to guess the mean of the SalePrice column. For the variables baseline_train and baseline_test, create arrays of same shape as y_train and y_test respectively. The variable baseline_train should contain y_train.mean(). The variable baseline_test should contain y_test.mean().\n",
        "\n",
        "Use the mean_squared_error function to calculate the error between baseline_train and y_train, Assign the result to mse_baseline_train.\n",
        "\n",
        "Use the mean_squared_error function to calculate the error between baseline_test and y_test, Assign the result to mse_baseline_test."
      ],
      "metadata": {
        "id": "iBbcECm3hFtH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### GRADED\n",
        "\n",
        "baseline_train = np.full_like(y_train, y_train.mean())\n",
        "baseline_test =  np.full_like(y_test, y_test.mean())\n",
        "\n",
        "mse_baseline_train = mean_squared_error(y_train, baseline_train)\n",
        "mse_baseline_test  = mean_squared_error(y_test, baseline_test)\n",
        "\n",
        "# YOUR CODE HERE\n",
        "#raise NotImplementedError()\n",
        "\n",
        "# Answer check\n",
        "print(baseline_train.shape, baseline_test.shape)\n",
        "print(f'Baseline for training data: {mse_baseline_train}')\n",
        "print(f'Baseline for testing data: {mse_baseline_test}')"
      ],
      "metadata": {
        "id": "TRD4XkWoZ3zg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Examining the Correlations\n",
        "\n",
        "What feature has the highest positive correlation with SalePrice? Assign your answer as a string matching the column name exactly to highest_corr below."
      ],
      "metadata": {
        "id": "CoXtnSCGhKsC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### GRADED\n",
        "\n",
        "#highest_corr =\n",
        "\n",
        "def find_highest_correlated_feature(df, target_column):\n",
        "\n",
        "    # Calculate the correlation matrix\n",
        "    correlation_matrix = df.corr()\n",
        "\n",
        "    # Get correlations with the target column\n",
        "    correlations = correlation_matrix[target_column].drop(target_column)\n",
        "\n",
        "    # Find the feature with the highest positive correlation\n",
        "    positive_correlations = correlations[correlations > 0]\n",
        "    if not positive_correlations.empty:\n",
        "        highest_correlated_feature = positive_correlations.idxmax()\n",
        "        return highest_correlated_feature\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "\n",
        "# Example Usage:\n",
        "# Assuming you have a DataFrame called 'data' and 'SalePrice' is the target column\n",
        "# Replace 'your_data.csv' with the actual path to your data file\n",
        "\n",
        "# Call the function\n",
        "highest_correlated = find_highest_correlated_feature(train, \"SalePrice\")\n",
        "\n",
        "# Print the result\n",
        "if highest_correlated:\n",
        "    print(f\"The feature with the highest positive correlation to SalePrice is: {highest_correlated}\")\n",
        "else:\n",
        "    print(\"No positive correlations found with SalePrice.\")\n",
        "\n",
        "highest_corr = highest_correlated\n",
        "\n",
        "# YOUR CODE HERE\n",
        "#raise NotImplementedError()\n",
        "\n",
        "# Answer check\n",
        "print(highest_corr)"
      ],
      "metadata": {
        "id": "7OxzPjYXZ6Xk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Simple Model\n",
        "\n",
        "Complete the code below according to the instructions below:\n",
        "\n",
        "1. Define a variable X1 and assign to it the values in the column OverallQual.\n",
        "2. Instantiate a LinearRegression model and use the fit function to train it using X1 and y_train. Assing your result to lr.\n",
        "3. Use the mean_squared_error function to calculate the error between y_train and lr.predict(X1). Assign the result to model_1_train_mse.\n",
        "4. Use the mean_squared_error function to calculate the error between y_test and lr.predict(X_test[['OverallQual']]. Assign the result to model_1_test_mse."
      ],
      "metadata": {
        "id": "PEzDIa_EhQJV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### GRADED\n",
        "\n",
        "X1 = X_train[['OverallQual']]\n",
        "\n",
        "lr = LinearRegression().fit(X1, y_train)\n",
        "\n",
        "model_1_train_mse = mean_squared_error(y_train, lr.predict(X1))\n",
        "model_1_test_mse =  mean_squared_error(y_test, lr.predict(X_test[['OverallQual']]))\n",
        "\n",
        "# YOUR CODE HERE\n",
        "#raise NotImplementedError()\n",
        "\n",
        "# Answer check\n",
        "print(f'Train MSE: {model_1_train_mse: .2f}')\n",
        "print(f'Test MSE: {model_1_test_mse: .2f}')"
      ],
      "metadata": {
        "id": "F_7zLgXmZ-Ok"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem 5\n",
        "# Using OneHotEncoder\n",
        "\n",
        "Similar to the pd.get_dummies() method earlier encountered, scikit-learn has a utility for encoding categorical features in the same way. Below, the OneHotEncoder is demonstrated in the CentralAir column. You are to use these results to build a model where the only feature is the CentralAir column. Note the two arguments are used in the OneHotEncoder:\n",
        "\n",
        "1. sparse = False: returns an array that we can investigate vs with sparse = True you are returned a sparse matrix -- a memory saving representation\n",
        "2. drop = if_binary: returns a single column for any binary categories. This avoids redundant features in our regression model.\n",
        "In the code cell below, instantiate a LinearRegression model and use the fit function to train it using model_2_train and y_train. Assing your result to model_2."
      ],
      "metadata": {
        "id": "dkV9YijGhXFZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#extract the features\n",
        "central_air_train = X_train[['CentralAir']]\n",
        "central_air_test  = X_test[['CentralAir']]\n",
        "#a categorical feature\n",
        "central_air_train.head()"
      ],
      "metadata": {
        "id": "rPcMWSKbaAsc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Instantiate a OHE object\n",
        "#sparse = False returns an array so we can view\n",
        "ohe = OneHotEncoder(sparse = False, drop='if_binary')\n",
        "print(ohe.fit_transform(central_air_train)[:7])\n",
        "#ohe"
      ],
      "metadata": {
        "id": "dKf0LtgzaDGI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_2_train = ohe.fit_transform(central_air_train)\n",
        "model_2_test = ohe.transform(central_air_test)"
      ],
      "metadata": {
        "id": "6iCf8OrCaHtk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### GRADED\n",
        "\n",
        "#In the code cell below, instantiate a LinearRegression model\n",
        "#and use the fit function to train it using model_2_train and y_train. Assing your result to model_2.\n",
        "model_2 = LinearRegression().fit(model_2_train, y_train )\n",
        "\n",
        "# YOUR CODE HERE\n",
        "#raise NotImplementedError()\n",
        "\n",
        "# Answer check\n",
        "print(model_2.coef_)"
      ],
      "metadata": {
        "id": "whar-MKRaJJ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To build a model using both the 'OverallQual' column and the 'CentralAir' column, you could use the OneHotEncoder to transform CentralAir, and then concatenate the results back into a DataFrame or numpy array. To streamline this process, the make_column_transformer can be used to seperate specific columns for certain transformations. Below, a make_column_transformer has been created for you to do just this.\n",
        "\n",
        "The arguments are tuples of the form (transformer, columns) that specify a transformation to perform on the given column. Further, the remainder = passthrough argument says to just pass the other columns through. You are returned a numpy array with the CentralAir column binarized and concatenated to the OverallQual feature.\n",
        "\n",
        "For an example using the make_column_transformer see here."
      ],
      "metadata": {
        "id": "ye_eUTkwhqSC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "col_transformer = make_column_transformer((OneHotEncoder(drop = 'if_binary'), ['CentralAir']),\n",
        "                                          remainder='passthrough')\n",
        "#ohe = OneHotEncoder(sparse = False, drop='if_binary')\n",
        "col_transformer"
      ],
      "metadata": {
        "id": "6V0lrnSIaOPY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "col_transformer.fit_transform(X_train[['OverallQual', 'CentralAir']][:5])"
      ],
      "metadata": {
        "id": "3utpW_95aO1E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem 6\n",
        "# Using make_column_transformer\n",
        "10 Points\n",
        "\n",
        "Complete the code below according to the instructions below:\n",
        "\n",
        "1. Use Pipeline to create a pipeline object. Inside the pipeline object, define a tuple where the first element is a string identifier col_transformer and the second element is an instance of col_transformer. Inside the pipeline define another tuple where the first element is a string identifier linreg, and the second element is an instance of LinearRegression. Assign the pipeline object to the variable pipe_1.\n",
        "\n",
        "2. Use the fit function on pipe_1 to train your model on X_train[['OverallQual','CentralAir']] and y_train."
      ],
      "metadata": {
        "id": "gb1tQV93hxyD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### GRADED\n",
        "\n",
        "pipe_1 = Pipeline([\n",
        "    ('col_transformer',col_transformer),                         #PolynomialFeatures(degree=2))\n",
        "    ('linreg',LinearRegression())\n",
        "])\n",
        "\n",
        "pipe_1.fit(X_train[['OverallQual','CentralAir']], y_train)\n",
        "# YOUR CODE HERE\n",
        "#raise NotImplementedError()\n",
        "\n",
        "# Answer check\n",
        "print(pipe_1.named_steps)#col_transformer and linreg should be keys\n",
        "pipe_1"
      ],
      "metadata": {
        "id": "q209Wl3uaQM0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Not all columns warrant binarization as done on the CentralAir column. For example, consider the HeatingQC feature -- representing the quality of the heating in the house. From the data description, the unique values are described as:\n",
        "\n",
        "HeatingQC: Heating quality and condition\n",
        "\n",
        "       Ex    Excellent\n",
        "       Gd    Good\n",
        "       TA    Average/Typical\n",
        "       Fa    Fair\n",
        "       Po    Poor\n",
        "These are ordered values, and rather than binarizing them a numeric value representing the scale can be used. For example, using a scale of 0 - 4 you may associate the categories with an order in a list from least to greatest as:\n",
        "\n",
        "['Po',          'Fa',        'TA',         'Gd',       'Ex']\n",
        "\n",
        "Creating an OrdinalEncoder with these categories will transform the HeatingQC feature mapping each category as\n",
        "\n",
        "\n",
        "*   Po:    0\n",
        "*   Fa:    1\n",
        "*   TA:    2\n",
        "*   Gd:    3\n",
        "*   Ex:    4\n",
        "\n",
        "This is demonstrated below, and in a similar manner, the use of the make_column_transformer is shown using the three columns ['OverallQual', 'CentralAir', 'HeatingQC'], applying the appropriate transformations to each column and passing the remaining numeric feature through."
      ],
      "metadata": {
        "id": "oQdu6WrIh5la"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "oe = OrdinalEncoder(categories = [['Po', 'Fa', 'TA', 'Gd', 'Ex']])\n",
        "oe.fit_transform(X_train[['HeatingQC']])\n",
        "X_train['HeatingQC'].head()"
      ],
      "metadata": {
        "id": "yMjJLcG2aSEm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ordinal_ohe_transformer = make_column_transformer((OneHotEncoder(drop = 'if_binary'), ['CentralAir']),\n",
        "                                          (OrdinalEncoder(categories = [['Po', 'Fa', 'TA', 'Gd', 'Ex']]), ['HeatingQC']),\n",
        "                                          remainder='passthrough')"
      ],
      "metadata": {
        "id": "psml2JijabFT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ordinal_ohe_transformer.fit_transform(X_train[['OverallQual', 'CentralAir', 'HeatingQC']])[:5]"
      ],
      "metadata": {
        "id": "TMHYR0oyac1Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train[['OverallQual', 'CentralAir', 'HeatingQC']].head()"
      ],
      "metadata": {
        "id": "8c4pJZhUaeR8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using OrdinalEncoder\n",
        "\n",
        "10 Points\n",
        "\n",
        "Complete the code below according to the instructions below:\n",
        "\n",
        "1. Use Pipeline to create a pipeline object. Inside the pipeline object define a tuple where the first element is a string identifier transformer and the second element is an instance of ordinal_ohe_transformer. Inside the pipeline define another tuple where the first element is a string identifier linreg, and the second element is an instance of LinearRegression. Assign the pipeline object to the variable pipe_2.\n",
        "2. Use the fit function on pipe_2 to train your model on X_train[['OverallQual', 'CentralAir', 'HeatingQC']] and y_train.\n",
        "3. Use the predict function on pipe_2 to make your predictions of X_train[['OverallQual', 'CentralAir', 'HeatingQC']]. Assign the result to pred_train.\n",
        "4. Use the predict function on pipe_2 to make your predictions of X_test[['OverallQual', 'CentralAir', 'HeatingQC']]. Assign the result to pred_test.\n",
        "5. Use the mean_squared_error function to calculate the MSE between y_train and pred_train. Assign the result to pipe_2_train_mse.\n",
        "6. Use the mean_squared_error function to calculate the MSE between y_test and pred_test. Assign the result to pipe_2_test_mse."
      ],
      "metadata": {
        "id": "GggVP1kylNPF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### GRADED\n",
        "\n",
        "pipe_2 = Pipeline([\n",
        "            ('transformer',ordinal_ohe_transformer),\n",
        "            ('linreg',LinearRegression())\n",
        "])\n",
        "pipe_2.fit(X_train[['OverallQual', 'CentralAir', 'HeatingQC']], y_train)\n",
        "\n",
        "pred_train = pipe_2.predict(X_train[['OverallQual', 'CentralAir', 'HeatingQC']])\n",
        "pred_test  = pipe_2.predict(X_test[['OverallQual', 'CentralAir', 'HeatingQC']])\n",
        "\n",
        "pipe_2_train_mse = mean_squared_error(y_train, pred_train)\n",
        "pipe_2_test_mse  =  mean_squared_error(y_test, pred_test)\n",
        "\n",
        "# YOUR CODE HERE\n",
        "#raise NotImplementedError()\n",
        "\n",
        "# Answer check\n",
        "print(pipe_2.named_steps)\n",
        "print(f'Train MSE: {pipe_2_train_mse: .2f}')\n",
        "print(f'Test MSE: {pipe_2_test_mse: .2f}')\n",
        "pipe_2"
      ],
      "metadata": {
        "id": "rUvlg3aKafsK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Including PolynomialFeatures\n",
        "\n",
        "Finally, the earlier transformation of continuous columns using the PolynomialFeatures with degree = 2 can be implemented alongside the OneHotEncoder and OrdinalEncoder.\n",
        "\n",
        "The make_column_transformer is again used, and you are to create a Pipeline with steps transformer and linreg.\n",
        "\n",
        "The Pipeline is fit on the training data using features ['OverallQual', 'CentralAir', 'HeatingQC'].\n",
        "\n",
        "1. Use the predict function on pipe_3 to predict the values of X_train[['OverallQual', 'CentralAir', 'HeatingQC']]. Assign your result to quad_train_preds.\n",
        "2. Use the predict function on pipe_3 to predict the values of X_test[['OverallQual', 'CentralAir', 'HeatingQC']]. Assign your result to quad_test_preds.\n",
        "3. Use the mean_squared_error function to calculate the MSE between y_train and quad_train_preds. Assign the result to quad_train_mse.\n",
        "4. Use the mean_squared_error function to calculate the MSE between y_test and quad_test_preds. Assign the result to quad_test_mse."
      ],
      "metadata": {
        "id": "85zPGuoTln2i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "poly_ordinal_ohe = make_column_transformer((OrdinalEncoder(categories = [['Po', 'Fa', 'TA', 'Gd', 'Ex']]), ['HeatingQC']),\n",
        "                                           (OneHotEncoder(drop = 'if_binary'), ['CentralAir']),\n",
        "                                           (PolynomialFeatures(include_bias = False, degree = 2), ['OverallQual']))\n",
        "poly_ordinal_ohe.fit_transform(X_train[['OverallQual','CentralAir', 'HeatingQC']])[:5]\n",
        "pipe_3 = Pipeline([('transformer', poly_ordinal_ohe),\n",
        "                  ('linreg', LinearRegression())])"
      ],
      "metadata": {
        "id": "RGLvXVBIah17"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipe_3.fit(X_train[['OverallQual', 'CentralAir', 'HeatingQC']], y_train)"
      ],
      "metadata": {
        "id": "zrokKsX7akEm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### GRADED\n",
        "\n",
        "quad_train_preds = pipe_3.predict(X_train[['OverallQual', 'CentralAir', 'HeatingQC']])\n",
        "quad_test_preds  = pipe_3.predict(X_test[['OverallQual', 'CentralAir', 'HeatingQC']])\n",
        "\n",
        "quad_train_mse = mean_squared_error(y_train, quad_train_preds)\n",
        "quad_test_mse =  mean_squared_error(y_test, quad_test_preds)\n",
        "\n",
        "# YOUR CODE HERE\n",
        "#raise NotImplementedError()\n",
        "\n",
        "# Answer check\n",
        "print(f'Train MSE: {quad_train_mse: .2f}')\n",
        "print(f'Test MSE: {quad_test_mse: .2f}')"
      ],
      "metadata": {
        "id": "UrV1MVLFalW_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Including More Features\n",
        "\n",
        "Use the following features to build a new make_column_transformer and fit 5 different models of degree 1 - 5 using the degree argument in your PolynomialFeatures transformer. Keep track of the subsequent train mean squared error and test set mean squared error with the lists train_mses and test_mses respectively.\n",
        "\n",
        "The poly_ordinal_ohe object contains the different transformers needed. Note that rather than passing a list of columns to the PolynomialFeatures transformer, the make_column_selector function is used to select any numeric feature. For more information on the make_column_selector see here."
      ],
      "metadata": {
        "id": "SURP_UgUl617"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "features = ['CentralAir', 'HeatingQC', 'OverallQual', 'GrLivArea', 'KitchenQual', 'FullBath']\n",
        "X_train[features].head()"
      ],
      "metadata": {
        "id": "muC3hEV0aoGw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "poly_ordinal_ohe = make_column_transformer((PolynomialFeatures(), make_column_selector(dtype_include=np.number)),\n",
        "                                           (OrdinalEncoder(categories = [['Po', 'Fa', 'TA', 'Gd', 'Ex']]), ['HeatingQC', 'KitchenQual']),\n",
        "                                               (OneHotEncoder(drop = 'if_binary', sparse = False), ['CentralAir']))"
      ],
      "metadata": {
        "id": "skcMHGwnasCN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### GRADED\n",
        "\n",
        "train_mses = []\n",
        "test_mses = []\n",
        "#for degree in 1 - 5\n",
        "for i in range(1, 6):\n",
        "    #create pipeline with PolynomialFeatures degree i\n",
        "\n",
        "    #ADD APPROPRIATE ARGUMENTS IN POLYNOMIALFEATURES\n",
        "    poly_ordinal_ohe = make_column_transformer((PolynomialFeatures(degree=i), make_column_selector(dtype_include=np.number)),\n",
        "                                           (OrdinalEncoder(categories = [['Po', 'Fa', 'TA', 'Gd', 'Ex']]), ['HeatingQC']),\n",
        "                                               (OneHotEncoder(drop = 'if_binary'), ['CentralAir']))\n",
        "\n",
        "    pipe_3 = Pipeline([('transformer', poly_ordinal_ohe),\n",
        "                  ('linreg', LinearRegression())\n",
        "                      ])\n",
        "\n",
        "    #fit on train\n",
        "    pipe_3.fit(X_train[features], y_train)\n",
        "\n",
        "    #predict on train and test\n",
        "    pipe_3.predict(X_train[features])\n",
        "    pipe_3.predict(X_test[features])\n",
        "    #compute mean squared errors\n",
        "   # train_mses  = mean_squared_error(y_train, quad_train_preds)\n",
        "   # test_mses   = mean_squared_error(y_train, quad_train_preds)\n",
        "    #append to train_mses and test_mses respectively\n",
        "    train_mses.append(mean_squared_error(y_train, pipe_3.predict(X_train[features])))\n",
        "    test_mses.append(mean_squared_error(y_test, pipe_3.predict(X_test[features])))\n",
        "\n",
        "# YOUR CODE HERE\n",
        "#raise NotImplementedError()\n",
        "\n",
        "# Answer check\n",
        "print(train_mses)\n",
        "print(test_mses)\n",
        "#pipe"
      ],
      "metadata": {
        "id": "zeSEYdK9atfR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Optimal Model Complexity\n",
        "\n",
        "Based on your model's mean squared error on the testing data in Problem 9 above, what was the optimal complexity? Assign your answer as an integer to best_complexity below. Compute the MEAN SQUARED ERROR of this model and assign it to best_mse as a float."
      ],
      "metadata": {
        "id": "uSvt74aumDAC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### GRADED\n",
        "\n",
        "best_complexity = 2\n",
        "\n",
        "poly_ordinal_ohe = make_column_transformer((PolynomialFeatures(degree=2), make_column_selector(dtype_include=np.number)),\n",
        "                                           (OrdinalEncoder(categories = [['Po', 'Fa', 'TA', 'Gd', 'Ex']]), ['HeatingQC']),\n",
        "                                               (OneHotEncoder(drop = 'if_binary'), ['CentralAir']))\n",
        "\n",
        "pipe_3 = Pipeline([('transformer', poly_ordinal_ohe),\n",
        "                  ('linreg', LinearRegression())\n",
        "                      ])\n",
        "\n",
        "    #fit on train\n",
        "pipe_3.fit(X_train[features], y_train)\n",
        "\n",
        "    #predict on train and test\n",
        "pipe_3.predict(X_train[features])\n",
        "pipe_3.predict(X_test[features])\n",
        "    #compute mean squared errors\n",
        "   # train_mses  = mean_squared_error(y_train, quad_train_preds)\n",
        "best_mse   = mean_squared_error(y_test, pipe_3.predict(X_test[features]))\n",
        "\n",
        "# YOUR CODE HERE\n",
        "#raise NotImplementedError()\n",
        "\n",
        "# Answer check\n",
        "print(f'The best degree polynomial model is:  {best_complexity}')\n",
        "print(f'The smallest mean squared error on the test data is : {best_mse: .2f}')"
      ],
      "metadata": {
        "id": "wKz5ua6xavhI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "d1ScxABkayT2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Further Exploration\n",
        "This activity was meant to introduce you to a more streamlined modeling process using the sklearn library. While your models should be performing better than the baseline, it is likely that with a bit more feature engineering and cross-validation you would be able to further improve the performance. You are encouraged to explore further feature engineering and encoding, particularly with handling missing values.\n",
        "\n",
        "Additionally, other transformations on the data may be appropriate. For example, if you look at the distribution of errors in your model, you will note that they are slightly skewed. An assumption of a Linear Regression model is that these should be roughly normally distributed. By building a model on the logarithm of the target column and evaluating the model on the logarithm of the testing data, you will improve towards this assumption. Note that the actual Kaggle exercise is judged on the ROOT MEAN SQUARED ERROR of the logarithm of the target feature.\n",
        "\n",
        "If interested, scikitlearn also provides a function TransformedTargetRegressor that will accomplish this transformation and can easily be added to a pipeline. See here for more information on this transformer."
      ],
      "metadata": {
        "id": "SmKD2OERmNJ8"
      }
    }
  ]
}